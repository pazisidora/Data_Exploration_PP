{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b3f08aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bokeh\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956da4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La siguiente funcion lee las 3 carpetas de informacion y crea 3 dataframes, luego a cada uno les realiza la ventana movil\n",
    "#para asi fusionarlos en un solo dataframe grande. Despues se escala una base solo con las variables numericas para finalmente \n",
    "#realizar los primeros modelos (regresion lineal y logistica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8c770808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para entrenamiento de los datos con regresión logística se tienen los siguientes resultados:\n",
      "t=134.9964450850342, p-value=0.0\n",
      "Valor del intercepto: [-1.53153775]\n",
      "Accuracy de entrenamiento para Reg Log: 99.4718554551772%\n",
      "\n",
      "Para testeo de los datos con regresión logística se tienen los siguientes resultados:\n",
      "El accuracy de testeo en Reg Log es: 99.30507296733843%\n",
      "\n",
      "Para entrenamiento de los datos con random forest:\n",
      "El error (rmse) de entrenamiento es: 0.007308900994923842\n",
      "Para testeo de los datos con random forest:\n",
      "El error (rmse) de test es: 0.030557706945832052\n"
     ]
    }
   ],
   "source": [
    "def probando(file1,file2,file3,delta_movil):\n",
    "    \n",
    "    ######Cargar bases NORMAL, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_normal_files=os.listdir(file1)\n",
    "    #Consolidando todas las bases con comportamiento normal en un DF\n",
    "    consolidated_normal_df = pd.DataFrame()\n",
    "    for name in names_normal_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = F'{file1}/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_normal_df = pd.concat([consolidated_normal_df,filename])\n",
    "    \n",
    "    #Asignando etiquetas\n",
    "    consolidated_normal_df['Label']=0\n",
    "    consolidated_normal_df['dataset']=\"Normal\"\n",
    "    \n",
    "    #Ventana movil\n",
    "    data_auxN=pd.DataFrame.copy(consolidated_normal_df)\n",
    "    data_auxN['Time'] = data_auxN.index if data_auxN.index.is_monotonic_increasing else range(len(data_auxN))\n",
    "    data_auxN.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxN2 = pd.DataFrame.copy(consolidated_normal_df)\n",
    "        data_auxN2['Time']= (data_auxN2.index if data_auxN2.index.is_monotonic_increasing else range(len(data_auxN2)))\n",
    "        data_auxN2['Time']=data_auxN2['Time']+j\n",
    "        data_auxN2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j), 'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxN3 = data_auxN.merge(data_auxN2, on='Time', how='inner')\n",
    "        data_auxN = pd.DataFrame.copy(data_auxN3)\n",
    "    #return data_auxN\n",
    "\n",
    "    \n",
    "    ######Cargar bases ATAQUE, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_attacked_files=os.listdir(file2) \n",
    "    #Consolidando todas las bases con comportamiento atacado en un DF\n",
    "    consolidated_attacked_df = pd.DataFrame()\n",
    "    for name in names_attacked_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = f'Attacked_Operation/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_attacked_df = pd.concat([consolidated_attacked_df,filename])\n",
    "    \n",
    "    #Funcion para etiquetar si la medición esta bajo ataque o no (sirve para este item como para las bases desbalanceadas)\n",
    "    def label_data(time):\n",
    "        if time <= 0.3:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    consolidated_attacked_df['Label']=consolidated_attacked_df['Time'].apply(label_data)\n",
    "\n",
    "    #Por temas de testeo, no necesitamos datos despues de los 0,5 segundos por lo que deben ser removidos.\n",
    "    consolidated_attacked_df.reset_index(drop=True,inplace=True)\n",
    "    boolean_mask = consolidated_attacked_df['Time']>0.5\n",
    "    attacked_index = consolidated_attacked_df[boolean_mask].index\n",
    "    consolidated_attacked_df.drop(attacked_index, axis=0,inplace=True)\n",
    "    \n",
    "    #Ahora etiqueto que es una base con ataque\n",
    "    consolidated_attacked_df['dataset'] = \"Attacked\"\n",
    "\n",
    "    #Ventana móvil\n",
    "    data_auxA=pd.DataFrame.copy(consolidated_attacked_df)\n",
    "    data_auxA['Time'] = data_auxA.index if data_auxA.index.is_monotonic_increasing else range(len(data_auxA))\n",
    "    data_auxA.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxA2 = pd.DataFrame.copy(consolidated_attacked_df)\n",
    "        data_auxA2['Time']= (data_auxA2.index if data_auxA2.index.is_monotonic_increasing else range(len(data_auxA2)))\n",
    "        data_auxA2['Time']=data_auxA2['Time']+j\n",
    "        data_auxA2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j),'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxA3 = data_auxA.merge(data_auxA2, on='Time', how='inner')\n",
    "        data_auxA = pd.DataFrame.copy(data_auxA3)\n",
    "    #return data_auxA\n",
    "    \n",
    "    ######Cargar bases DESBALANCEADAS ATACADAS, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_imbalanced_files=os.listdir(file3)\n",
    "    #Consolidando todas las bases desbalanceadas con comportamiento atacado en un DF\n",
    "    consolidated_imbalanced_df = pd.DataFrame()\n",
    "    for name in names_imbalanced_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = f'Imbalanced_Attacked/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_imbalanced_df = pd.concat([consolidated_imbalanced_df,filename])\n",
    "    \n",
    "    #Aplico etiqueta si esta bajo ataque o no\n",
    "    consolidated_imbalanced_df['Label']=consolidated_imbalanced_df['Time'].apply(label_data)\n",
    "    \n",
    "    #Por temas de testeo, no necesitamos datos despues de los 0,5 segundos por lo que deben ser removidos.\n",
    "    consolidated_imbalanced_df.reset_index(drop=True,inplace=True)\n",
    "    boolean_im_mask = consolidated_imbalanced_df['Time']>0.5\n",
    "    imbalanced_index = consolidated_imbalanced_df[boolean_im_mask].index\n",
    "    consolidated_imbalanced_df.drop(imbalanced_index,axis=0,inplace=True)\n",
    "    \n",
    "    #Ahora etiqueto que es una base desbalanceada con ataque\n",
    "    consolidated_imbalanced_df['dataset'] = \"Imbalanced\"\n",
    "    \n",
    "    #Ventana móvil\n",
    "    data_auxIA=pd.DataFrame.copy(consolidated_imbalanced_df)\n",
    "    data_auxIA['Time'] = data_auxIA.index if data_auxIA.index.is_monotonic_increasing else range(len(data_auxIA))\n",
    "    data_auxIA.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxIA2 = pd.DataFrame.copy(consolidated_imbalanced_df)\n",
    "        data_auxIA2['Time']= (data_auxIA2.index if data_auxIA2.index.is_monotonic_increasing else range(len(data_auxIA2)))\n",
    "        data_auxIA2['Time']=data_auxIA2['Time']+j\n",
    "        data_auxIA2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j),'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxIA3 = data_auxIA.merge(data_auxIA2, on='Time', how='inner')\n",
    "        data_auxIA = pd.DataFrame.copy(data_auxIA3)\n",
    "    #return data_auxIA    \n",
    "\n",
    "    ###### Consolidando todos los dataframes, con ventana de tiempo, en uno--------------------------------------------------------\n",
    "    consolidated_df = pd.concat([data_auxN, data_auxA]) #bases normales con atacadas\n",
    "    consolidated_df = pd.concat([consolidated_df, data_auxIA]) #lo anterior + desbalanceado\n",
    "    #print(consolidated_df.Label.value_counts())\n",
    "    \n",
    "    #####Reseteando los indices\n",
    "    consolidated_df.reset_index(drop=True,inplace=True)\n",
    "    #print(consolidated_df)\n",
    "    \n",
    "    \n",
    "    #ESTO ES LA PRUEBA TRATANDO DE ESCALAR, EN LA SIGUIENTE LINEA DE CODIGO VEREMOS QUE PASA CON EL MODELO SIN ESCALAR.\n",
    "    \n",
    "    \n",
    "    ##### Preporcesamiento de la data\n",
    "    #Importando para escalar\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler=StandardScaler()\n",
    "    \n",
    "    #Solo se necesita escalar las variables numericas, asi que se realiza eso\n",
    "    numerical_consolidated_df=consolidated_df.drop(['Time','Label','dataset'],axis=1)\n",
    "    \n",
    "    numerical_consolidated_df2=pd.DataFrame.copy(numerical_consolidated_df)\n",
    "    \n",
    "    for j in range(1,delta_movil+1):\n",
    "        numerical_consolidated_df2=numerical_consolidated_df2.drop(['dataset_'+str(j),'Label_'+str(j)],axis=1)\n",
    "    #return numerical_consolidated_df2\n",
    "    \n",
    "    #Escalar solo los valores numericos y colocar de vuelta a un df lo que era string\n",
    "    scaled_consolidated_df=scaler.fit_transform(numerical_consolidated_df2)\n",
    "    scaled_consolidated_df = pd.DataFrame(scaled_consolidated_df,columns=numerical_consolidated_df2.columns)\n",
    "    #print(scaled_consolidated_df)\n",
    "    \n",
    "    #Volver a concatenar las columnas de string\n",
    "    consolidated_df2=pd.concat([scaled_consolidated_df,consolidated_df[['Time','Label','dataset']]],axis=1)\n",
    "    #print(consolidated_df2)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        consolidated_df2=pd.concat([consolidated_df2,consolidated_df[['dataset_'+str(j),'Label_'+str(j)]]],axis=1)\n",
    "    #return consolidated_df2\n",
    "    \n",
    "    \n",
    "    ######### Testing First models -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    #Analizando un poco de forma grafica:\n",
    "    #fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "    #sns.violinplot(\n",
    "    #    x     = 'Label',\n",
    "     #   y     = 'V3',\n",
    "      #  data  = consolidated_df2,\n",
    "        #color = \"white\",\n",
    "       # ax = ax\n",
    "    #)\n",
    "\n",
    "    #ax.set_title('probando');\n",
    "    from statsmodels.stats.weightstats import ttest_ind\n",
    "    res_ttest = ttest_ind(\n",
    "                    x1 = consolidated_df2.V3[consolidated_df2.Label == 0],\n",
    "                    x2 = consolidated_df2.V3[consolidated_df2.Label == 1],\n",
    "                    alternative='two-sided'\n",
    "                )\n",
    "    print(\"Para entrenamiento de los datos con regresión logística se tienen los siguientes resultados:\")\n",
    "    print(f\"t={res_ttest[0]}, p-value={res_ttest[1]}\") #viendo el valor t y p\n",
    "    \n",
    "    ##### Train, test, split the data -----------------------------------------------------------------------------------------\n",
    "    X=scaled_consolidated_df \n",
    "    y=consolidated_df2['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=100)\n",
    "    \n",
    "    ##Regresion LOGISTICA:\n",
    "    \n",
    "    #Creacion y entrenamiento del modelo logístico\n",
    "    reg_log = LogisticRegression() #cargo el modelo de reg log\n",
    "    reg_log.fit(X_train, y_train) #Entreno el clasificador\n",
    "    \n",
    "    print(\"Valor del intercepto:\", reg_log.intercept_)\n",
    "    #print(\"Valores de los Coeficientes:\", list(zip(X.columns, reg_log.coef_.flatten(), )))\n",
    "    print(f\"Accuracy de entrenamiento para Reg Log: {100*(reg_log.score(X, y))}%\")\n",
    "    \n",
    "    #Prediccion\n",
    "    y_pred = reg_log.predict(X_train) #guardo las predicciones con el modelo entrenado\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Para testeo de los datos con regresión logística se tienen los siguientes resultados:\")\n",
    "    #Accuracy de testeo\n",
    "    #X_test = sm.add_constant(X_test, prepend=True) #no se en realidad si esto se debe agregar o no\n",
    "    predicciones = reg_log.predict(X_test)\n",
    "    clasificacion = np.where(predicciones<0.5, 0, 1)\n",
    "    accuracy = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = clasificacion,\n",
    "            normalize = True\n",
    "           )\n",
    "    print(f\"El accuracy de testeo en Reg Log es: {100*accuracy}%\")\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    ##RANDOM FOREST:\n",
    "    \n",
    "    #Creacion y entrenamiento del modelo Random Forest\n",
    "    random_forest = RandomForestRegressor(\n",
    "            n_estimators = 100,\n",
    "            max_depth    = None,\n",
    "            max_features = 'auto',\n",
    "            oob_score    = False,\n",
    "            n_jobs       = -1,\n",
    "            random_state = 100\n",
    "         )\n",
    "\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    #Error cuadratico medio - entrenamiento\n",
    "    pred_entr=random_forest.predict(X=X_train)\n",
    "    rmse_entr = mean_squared_error(\n",
    "        y_true  = y_train,\n",
    "        y_pred  = pred_entr,\n",
    "        squared = False\n",
    "       )\n",
    "    print(\"Para entrenamiento de los datos con random forest:\")\n",
    "    print(f\"El error (rmse) de entrenamiento es: {rmse_entr}\")\n",
    "    #print(f\"Accuracy de entrenamiento para Random Forest: {100*(random_forest.score(X, y))}%\")\n",
    "    \n",
    "    #Error cuadratico medio - testeo\n",
    "    pred_test = random_forest.predict(X = X_test)\n",
    "\n",
    "    rmse_test = mean_squared_error(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = pred_test,\n",
    "        squared = False\n",
    "       )\n",
    "    print(\"Para testeo de los datos con random forest:\")\n",
    "    print(f\"El error (rmse) de test es: {rmse_test}\")\n",
    "    \n",
    "    \n",
    "    ## SUPPORT VECTOR MACHINE (SVM)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #AAAAAAAAAAAAAAAAAAAAAAAA\n",
    "    \n",
    "    \n",
    "    ## K MEANS:\n",
    "    #KNN_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    #KNN_model.fit(X_train, y_train)\n",
    "    #KNN_prediction = KNN_model.predict(X_test)\n",
    "    \n",
    "    #Accuracy score is the simplest way to evaluate\n",
    "    #print(accuracy_score(y_pred, y_test))\n",
    "    #Confusion Matrix and Classification Report give more details about performance\n",
    "    #print(classification_report(y_pred, y_test))\n",
    "    \n",
    "    #matrix=confusion_matrix(y_test,KNN_prediction)\n",
    "    #ax= plt.subplot()\n",
    "    #sns.heatmap(matrix, annot=True, fmt='g', ax=ax);\n",
    "    # labels, title and ticks\n",
    "    #ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    #ax.set_title('Confusion Matrix'); \n",
    "    #ax.xaxis.set_ticklabels(['attack', 'normal']); ax.yaxis.set_ticklabels(['normal', 'attack']);\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "probando(\"Normal_Operation\",\"Attacked_Operation\",\"Imbalanced_Attacked\",2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#El codigo a continuacion es la misma funcion de arriba pero sin escalar la base numerica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "628325ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    11173\n",
      "1     3200\n",
      "Name: Label, dtype: int64\n",
      "Coefficients: \n",
      " [-1.81905765e+00 -5.74867264e+08  5.74867265e+08  8.42763475e-02\n",
      " -3.20460526e+10  3.20460526e+10 -3.22911558e-02  3.84607490e+00\n",
      "  7.39798348e+08 -7.39798350e+08  9.97191564e-02 -1.87638658e+08\n",
      "  1.87638658e+08  5.87802352e-02 -3.56903761e+00  1.88043560e+09\n",
      " -1.88043560e+09  5.19539755e-02  4.87145562e+10 -4.87145562e+10\n",
      "  6.36177951e-03  1.50300389e+00 -2.06788969e+09  2.06788969e+09\n",
      " -8.40952367e-03 -2.96283224e+10  2.96283224e+10  6.01154474e-03]\n",
      "Mean squared error: 0.01\n",
      "Coeficiente de determinación R^2 con reg lineal: 0.9526887232710781\n",
      "Coeficiente de determinación R^2 con reg logistica: 0.9954080567731163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pazno\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "def probando(file1,file2,file3,delta_movil):\n",
    "    \n",
    "    ######Cargar bases NORMAL, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_normal_files=os.listdir(file1)\n",
    "    #Consolidando todas las bases con comportamiento normal en un DF\n",
    "    consolidated_normal_df = pd.DataFrame()\n",
    "    for name in names_normal_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = F'{file1}/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_normal_df = pd.concat([consolidated_normal_df,filename])\n",
    "    \n",
    "    #Asignando etiquetas\n",
    "    consolidated_normal_df['Label']=0\n",
    "    consolidated_normal_df['dataset']=\"Normal\"\n",
    "    \n",
    "    #Ventana movil\n",
    "    data_auxN=pd.DataFrame.copy(consolidated_normal_df)\n",
    "    data_auxN['Time'] = data_auxN.index if data_auxN.index.is_monotonic_increasing else range(len(data_auxN))\n",
    "    data_auxN.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxN2 = pd.DataFrame.copy(consolidated_normal_df)\n",
    "        data_auxN2['Time']= (data_auxN2.index if data_auxN2.index.is_monotonic_increasing else range(len(data_auxN2)))\n",
    "        data_auxN2['Time']=data_auxN2['Time']+j\n",
    "        data_auxN2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j), 'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxN3 = data_auxN.merge(data_auxN2, on='Time', how='inner')\n",
    "        data_auxN = pd.DataFrame.copy(data_auxN3)\n",
    "    #return data_auxN\n",
    "\n",
    "    \n",
    "    ######Cargar bases ATAQUE, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_attacked_files=os.listdir(file2) \n",
    "    #Consolidando todas las bases con comportamiento atacado en un DF\n",
    "    consolidated_attacked_df = pd.DataFrame()\n",
    "    for name in names_attacked_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = f'Attacked_Operation/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_attacked_df = pd.concat([consolidated_attacked_df,filename])\n",
    "    \n",
    "    #Funcion para etiquetar si la medición esta bajo ataque o no (sirve para este item como para las bases desbalanceadas)\n",
    "    def label_data(time):\n",
    "        if time <= 0.3:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    consolidated_attacked_df['Label']=consolidated_attacked_df['Time'].apply(label_data)\n",
    "\n",
    "    #Por temas de testeo, no necesitamos datos despues de los 0,5 segundos por lo que deben ser removidos.\n",
    "    consolidated_attacked_df.reset_index(drop=True,inplace=True)\n",
    "    boolean_mask = consolidated_attacked_df['Time']>0.5\n",
    "    attacked_index = consolidated_attacked_df[boolean_mask].index\n",
    "    consolidated_attacked_df.drop(attacked_index, axis=0,inplace=True)\n",
    "    \n",
    "    #Ahora etiqueto que es una base con ataque\n",
    "    consolidated_attacked_df['dataset'] = \"Attacked\"\n",
    "\n",
    "    #Ventana móvil\n",
    "    data_auxA=pd.DataFrame.copy(consolidated_attacked_df)\n",
    "    data_auxA['Time'] = data_auxA.index if data_auxA.index.is_monotonic_increasing else range(len(data_auxA))\n",
    "    data_auxA.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxA2 = pd.DataFrame.copy(consolidated_attacked_df)\n",
    "        data_auxA2['Time']= (data_auxA2.index if data_auxA2.index.is_monotonic_increasing else range(len(data_auxA2)))\n",
    "        data_auxA2['Time']=data_auxA2['Time']+j\n",
    "        data_auxA2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j),'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxA3 = data_auxA.merge(data_auxA2, on='Time', how='inner')\n",
    "        data_auxA = pd.DataFrame.copy(data_auxA3)\n",
    "    #return data_auxA\n",
    "    \n",
    "    ######Cargar bases DESBALANCEADAS ATACADAS, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_imbalanced_files=os.listdir(file3)\n",
    "    #Consolidando todas las bases desbalanceadas con comportamiento atacado en un DF\n",
    "    consolidated_imbalanced_df = pd.DataFrame()\n",
    "    for name in names_imbalanced_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = f'Imbalanced_Attacked/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_imbalanced_df = pd.concat([consolidated_imbalanced_df,filename])\n",
    "    \n",
    "    #Aplico etiqueta si esta bajo ataque o no\n",
    "    consolidated_imbalanced_df['Label']=consolidated_imbalanced_df['Time'].apply(label_data)\n",
    "    \n",
    "    #Por temas de testeo, no necesitamos datos despues de los 0,5 segundos por lo que deben ser removidos.\n",
    "    consolidated_imbalanced_df.reset_index(drop=True,inplace=True)\n",
    "    boolean_im_mask = consolidated_imbalanced_df['Time']>0.5\n",
    "    imbalanced_index = consolidated_imbalanced_df[boolean_im_mask].index\n",
    "    consolidated_imbalanced_df.drop(imbalanced_index,axis=0,inplace=True)\n",
    "    \n",
    "    #Ahora etiqueto que es una base desbalanceada con ataque\n",
    "    consolidated_imbalanced_df['dataset'] = \"Imbalanced\"\n",
    "    \n",
    "    #Ventana móvil\n",
    "    data_auxIA=pd.DataFrame.copy(consolidated_imbalanced_df)\n",
    "    data_auxIA['Time'] = data_auxIA.index if data_auxIA.index.is_monotonic_increasing else range(len(data_auxIA))\n",
    "    data_auxIA.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxIA2 = pd.DataFrame.copy(consolidated_imbalanced_df)\n",
    "        data_auxIA2['Time']= (data_auxIA2.index if data_auxIA2.index.is_monotonic_increasing else range(len(data_auxIA2)))\n",
    "        data_auxIA2['Time']=data_auxIA2['Time']+j\n",
    "        data_auxIA2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j),'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxIA3 = data_auxIA.merge(data_auxIA2, on='Time', how='inner')\n",
    "        data_auxIA = pd.DataFrame.copy(data_auxIA3)\n",
    "    #return data_auxIA    \n",
    "\n",
    "    ###### Consolidando todos los dataframes, con ventana de tiempo, en uno\n",
    "    consolidated_df = pd.concat([data_auxN, data_auxA]) #bases normales con atacadas\n",
    "    consolidated_df = pd.concat([consolidated_df, data_auxIA]) #lo anterior + desbalanceado\n",
    "    print(consolidated_df.Label.value_counts())\n",
    "    \n",
    "    #####Reseteando los indices\n",
    "    consolidated_df.reset_index(drop=True,inplace=True)\n",
    "    #print(consolidated_df)\n",
    "    \n",
    "    \n",
    "    #ESTO ES LA PRUEBA SIN ESCALAR\n",
    "    \n",
    "    numerical_consolidated_df=consolidated_df.drop(['Time','Label','dataset'],axis=1)\n",
    "    base_aux=pd.DataFrame.copy(numerical_consolidated_df)\n",
    "    \n",
    "    for j in range(1,delta_movil+1):\n",
    "        base_aux=base_aux.drop(['dataset_'+str(j),'Label_'+str(j)],axis=1)\n",
    "    #return base_aux\n",
    "    \n",
    "    \n",
    "    ##### Train, test, split the data\n",
    "    X=base_aux\n",
    "    y=consolidated_df['Label']\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    #Testing First models\n",
    "    ## LINEAR MODELS\n",
    "    from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "    regr = LinearRegression()\n",
    "    regr2=LogisticRegression()\n",
    "    \n",
    "    regr.fit(X_train, y_train)\n",
    "    regr2.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = regr.predict(X_train)\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    print(\"Mean squared error: %.2f\" % mean_squared_error(y_train, y_pred))\n",
    "    print(\"Coeficiente de determinación R^2 con reg lineal:\", regr.score(X, y))\n",
    "    print(\"Coeficiente de determinación R^2 con reg logistica:\", regr2.score(X, y))\n",
    "    \n",
    "    \n",
    "probando(\"Normal_Operation\",\"Attacked_Operation\",\"Imbalanced_Attacked\",3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9eeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esto es por si no funciona lo de arriba para escalar\n",
    "base_aux=pd.DataFrame()\n",
    "    base_aux2=pd.DataFrame()\n",
    "    base2_aux2=pd.DataFrame()\n",
    "    #for i in numerical_consolidated_df.index:\n",
    "    #for col in numerical_consolidated_df.columns:\n",
    "    for j in range(1,delta_movil+1):\n",
    "        base_aux=pd.DataFrame(columns=['dataset_'+str(j)])\n",
    "        base_aux2=pd.concat([base_aux2,base_aux])\n",
    "        base_aux3=pd.concat([base_aux2,numerical_consolidated_df])\n",
    "        \n",
    "        for col in base_aux3.columns:\n",
    "            str(col)\n",
    "            base_aux3.drop([col],axis=1)\n",
    "        \n",
    "        \n",
    "        #base2_aux=pd.DataFrame(columns=['Label_'+str(j)])\n",
    "        #base2_aux2=pd.concat([base2_aux2,base2_aux])\n",
    "        #base2_aux3=pd.concat([base2_aux2,numerical_consolidated_df])\n",
    "        #base_aux4=pd.concat([base2_aux3,base_aux3])\n",
    "        \n",
    "        #base_aux2.loc[j-1]=numerical_consolidated_df['dataset_'+str(j)]\n",
    "            #if col == 'dataset_'+str(j):\n",
    "                #numerical_consolidated_df=pd.concat([numerical_consolidated_df,numerical_consolidated_df[[col]]],axis=1)\n",
    "                #auxiliar=numerical_consolidated_df['Label_'+str(j),'dataset_'+str(j)]\n",
    "                #numerical_consolidated_df.drop([col],axis=1)\n",
    "    \n",
    "    return #base_aux3\n",
    "    \n",
    "   #invoices.drop(['invoice', 'client', 'units'], axis=1)\n",
    "    #for col in df.columns:\n",
    "    #if 'A' in col:\n",
    "     #   del df[col]\n",
    "    \n",
    "    #indexNames = df[ (df['Price'] >= 30)\n",
    "     #           & (df['Price'] <= 70) ].index\n",
    "#df.drop(indexNames , inplace=True)\n",
    "    \n",
    "# This will return false\n",
    "# As the value passed is string and you are checking it with int\n",
    "\n",
    "\n",
    "\n",
    "#extra cuando hice el primer pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "    regr = LinearRegression()\n",
    "    regr2=LogisticRegression()\n",
    "    \n",
    "    regr.fit(X_train, y_train)\n",
    "    regr2.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = regr.predict(X_train)\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    print(\"Mean squared error: %.2f\" % mean_squared_error(y_train, y_pred))\n",
    "    print(\"Coeficiente de determinación R^2 con reg lineal:\", regr.score(X, y))\n",
    "    print(\"Coeficiente de determinación R^2 con reg logistica:\", regr2.score(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
