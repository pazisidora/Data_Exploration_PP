{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f08aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bokeh\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486263cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La siguiente funcion lee las 3 carpetas de informacion y crea 3 dataframes, luego a cada uno les realiza la ventana movil\n",
    "#para asi fusionarlos en un solo dataframe grande. Despues se escala una base solo con las variables numericas para finalmente \n",
    "#realizar los primeros modelos (regresion lineal y logistica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c770808",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] El sistema no puede encontrar la ruta especificada: 'Normal_Operation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18448/2025083181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m \u001b[0mprobando\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Normal_Operation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Attacked_Operation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Imbalanced_Attacked\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18448/2025083181.py\u001b[0m in \u001b[0;36mprobando\u001b[1;34m(file1, file2, file3, delta_movil)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m######Cargar bases NORMAL, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mnames_normal_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m#Consolidando todas las bases con comportamiento normal en un DF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mconsolidated_normal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] El sistema no puede encontrar la ruta especificada: 'Normal_Operation'"
     ]
    }
   ],
   "source": [
    "def probando(file1,file2,file3,delta_movil):\n",
    "    \n",
    "    ######Cargar bases NORMAL, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_normal_files=os.listdir(file1)\n",
    "    #Consolidando todas las bases con comportamiento normal en un DF\n",
    "    consolidated_normal_df = pd.DataFrame()\n",
    "    for name in names_normal_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = F'{file1}/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_normal_df = pd.concat([consolidated_normal_df,filename])\n",
    "    \n",
    "    #Asignando etiquetas\n",
    "    consolidated_normal_df['Label']=0\n",
    "    consolidated_normal_df['dataset']=\"Normal\"\n",
    "    \n",
    "    #Ventana movil\n",
    "    data_auxN=pd.DataFrame.copy(consolidated_normal_df)\n",
    "    data_auxN['Time'] = data_auxN.index if data_auxN.index.is_monotonic_increasing else range(len(data_auxN))\n",
    "    data_auxN.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxN2 = pd.DataFrame.copy(consolidated_normal_df)\n",
    "        data_auxN2['Time']= (data_auxN2.index if data_auxN2.index.is_monotonic_increasing else range(len(data_auxN2)))\n",
    "        data_auxN2['Time']=data_auxN2['Time']+j\n",
    "        data_auxN2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j), 'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxN3 = data_auxN.merge(data_auxN2, on='Time', how='inner')\n",
    "        data_auxN = pd.DataFrame.copy(data_auxN3)\n",
    "    #return data_auxN\n",
    "\n",
    "    \n",
    "    ######Cargar bases ATAQUE, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_attacked_files=os.listdir(file2) \n",
    "    #Consolidando todas las bases con comportamiento atacado en un DF\n",
    "    consolidated_attacked_df = pd.DataFrame()\n",
    "    for name in names_attacked_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = f'Attacked_Operation/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_attacked_df = pd.concat([consolidated_attacked_df,filename])\n",
    "    \n",
    "    #Funcion para etiquetar si la medición esta bajo ataque o no (sirve para este item como para las bases desbalanceadas)\n",
    "    def label_data(time):\n",
    "        if time <= 0.3:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    consolidated_attacked_df['Label']=consolidated_attacked_df['Time'].apply(label_data)\n",
    "\n",
    "    #Por temas de testeo, no necesitamos datos despues de los 0,5 segundos por lo que deben ser removidos.\n",
    "    consolidated_attacked_df.reset_index(drop=True,inplace=True)\n",
    "    boolean_mask = consolidated_attacked_df['Time']>0.5\n",
    "    attacked_index = consolidated_attacked_df[boolean_mask].index\n",
    "    consolidated_attacked_df.drop(attacked_index, axis=0,inplace=True)\n",
    "    \n",
    "    #Ahora etiqueto que es una base con ataque\n",
    "    consolidated_attacked_df['dataset'] = \"Attacked\"\n",
    "\n",
    "    #Ventana móvil\n",
    "    data_auxA=pd.DataFrame.copy(consolidated_attacked_df)\n",
    "    data_auxA['Time'] = data_auxA.index if data_auxA.index.is_monotonic_increasing else range(len(data_auxA))\n",
    "    data_auxA.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxA2 = pd.DataFrame.copy(consolidated_attacked_df)\n",
    "        data_auxA2['Time']= (data_auxA2.index if data_auxA2.index.is_monotonic_increasing else range(len(data_auxA2)))\n",
    "        data_auxA2['Time']=data_auxA2['Time']+j\n",
    "        data_auxA2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j),'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxA3 = data_auxA.merge(data_auxA2, on='Time', how='inner')\n",
    "        data_auxA = pd.DataFrame.copy(data_auxA3)\n",
    "    #return data_auxA\n",
    "    \n",
    "    ######Cargar bases DESBALANCEADAS ATACADAS, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_imbalanced_files=os.listdir(file3)\n",
    "    #Consolidando todas las bases desbalanceadas con comportamiento atacado en un DF\n",
    "    consolidated_imbalanced_df = pd.DataFrame()\n",
    "    for name in names_imbalanced_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = f'Imbalanced_Attacked/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_imbalanced_df = pd.concat([consolidated_imbalanced_df,filename])\n",
    "    \n",
    "    #Aplico etiqueta si esta bajo ataque o no\n",
    "    consolidated_imbalanced_df['Label']=consolidated_imbalanced_df['Time'].apply(label_data)\n",
    "    \n",
    "    #Por temas de testeo, no necesitamos datos despues de los 0,5 segundos por lo que deben ser removidos.\n",
    "    consolidated_imbalanced_df.reset_index(drop=True,inplace=True)\n",
    "    boolean_im_mask = consolidated_imbalanced_df['Time']>0.5\n",
    "    imbalanced_index = consolidated_imbalanced_df[boolean_im_mask].index\n",
    "    consolidated_imbalanced_df.drop(imbalanced_index,axis=0,inplace=True)\n",
    "    \n",
    "    #Ahora etiqueto que es una base desbalanceada con ataque\n",
    "    consolidated_imbalanced_df['dataset'] = \"Imbalanced\"\n",
    "    \n",
    "    #Ventana móvil\n",
    "    data_auxIA=pd.DataFrame.copy(consolidated_imbalanced_df)\n",
    "    data_auxIA['Time'] = data_auxIA.index if data_auxIA.index.is_monotonic_increasing else range(len(data_auxIA))\n",
    "    data_auxIA.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxIA2 = pd.DataFrame.copy(consolidated_imbalanced_df)\n",
    "        data_auxIA2['Time']= (data_auxIA2.index if data_auxIA2.index.is_monotonic_increasing else range(len(data_auxIA2)))\n",
    "        data_auxIA2['Time']=data_auxIA2['Time']+j\n",
    "        data_auxIA2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j),'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxIA3 = data_auxIA.merge(data_auxIA2, on='Time', how='inner')\n",
    "        data_auxIA = pd.DataFrame.copy(data_auxIA3)\n",
    "    #return data_auxIA    \n",
    "\n",
    "    ###### Consolidando todos los dataframes, con ventana de tiempo, en uno--------------------------------------------------------\n",
    "    consolidated_df = pd.concat([data_auxN, data_auxA]) #bases normales con atacadas\n",
    "    consolidated_df = pd.concat([consolidated_df, data_auxIA]) #lo anterior + desbalanceado\n",
    "    #print(consolidated_df.Label.value_counts())\n",
    "    \n",
    "    #####Reseteando los indices\n",
    "    consolidated_df.reset_index(drop=True,inplace=True)\n",
    "    #print(consolidated_df)\n",
    "    \n",
    "    \n",
    "    #ESTO ES LA PRUEBA TRATANDO DE ESCALAR, EN LA SIGUIENTE LINEA DE CODIGO VEREMOS QUE PASA CON EL MODELO SIN ESCALAR.\n",
    "    \n",
    "    \n",
    "    ##### Preporcesamiento de la data\n",
    "    #Importando para escalar\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler=StandardScaler()\n",
    "    \n",
    "    #Solo se necesita escalar las variables numericas, asi que se realiza eso\n",
    "    numerical_consolidated_df=consolidated_df.drop(['Time','Label','dataset'],axis=1)\n",
    "    \n",
    "    numerical_consolidated_df2=pd.DataFrame.copy(numerical_consolidated_df)\n",
    "    \n",
    "    for j in range(1,delta_movil+1):\n",
    "        numerical_consolidated_df2=numerical_consolidated_df2.drop(['dataset_'+str(j),'Label_'+str(j)],axis=1)\n",
    "    #return numerical_consolidated_df2\n",
    "    \n",
    "    #Escalar solo los valores numericos y colocar de vuelta a un df lo que era string\n",
    "    scaled_consolidated_df=scaler.fit_transform(numerical_consolidated_df2)\n",
    "    scaled_consolidated_df = pd.DataFrame(scaled_consolidated_df,columns=numerical_consolidated_df2.columns)\n",
    "    #print(scaled_consolidated_df)\n",
    "    \n",
    "    #Volver a concatenar las columnas de string\n",
    "    consolidated_df2=pd.concat([scaled_consolidated_df,consolidated_df[['Time','Label','dataset']]],axis=1)\n",
    "    #print(consolidated_df2)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        consolidated_df2=pd.concat([consolidated_df2,consolidated_df[['dataset_'+str(j),'Label_'+str(j)]]],axis=1)\n",
    "    #return consolidated_df2\n",
    "    \n",
    "    \n",
    "    ######### Testing First models -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "    from statsmodels.stats.weightstats import ttest_ind\n",
    "    res_ttest = ttest_ind(\n",
    "                    x1 = consolidated_df2.V3[consolidated_df2.Label == 0],\n",
    "                    x2 = consolidated_df2.V3[consolidated_df2.Label == 1],\n",
    "                    alternative='two-sided'\n",
    "                )\n",
    "    print(\"Para entrenamiento de los datos con regresión logística se tienen los siguientes resultados:\")\n",
    "    print(f\"t={res_ttest[0]}, p-value={res_ttest[1]}\") #viendo el valor t y p\n",
    "    \n",
    "    ##### Train, test, split the data -----------------------------------------------------------------------------------------\n",
    "    X=scaled_consolidated_df \n",
    "    y=consolidated_df2['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=100)\n",
    "    \n",
    "    ##Regresion LOGISTICA:\n",
    "    \n",
    "    #Creacion y entrenamiento del modelo logístico\n",
    "    reg_log = LogisticRegression() #cargo el modelo de reg log\n",
    "    reg_log.fit(X_train, y_train) #Entreno el clasificador\n",
    "    \n",
    "    print(\"Valor del intercepto:\", reg_log.intercept_)\n",
    "    #print(\"Valores de los Coeficientes:\", list(zip(X.columns, reg_log.coef_.flatten(), )))\n",
    "    print(f\"Accuracy de entrenamiento para Reg Log: {100*(reg_log.score(X, y))}%\")\n",
    "    \n",
    "    #Prediccion\n",
    "    y_pred = reg_log.predict(X_train) #guardo las predicciones con el modelo entrenado\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Para testeo de los datos con regresión logística se tienen los siguientes resultados:\")\n",
    "    #Accuracy de testeo\n",
    "    #X_test = sm.add_constant(X_test, prepend=True) #no se en realidad si esto se debe agregar o no\n",
    "    pred_test_log = reg_log.predict(X_test)\n",
    "    clasificacion = np.where(pred_test_log<0.5, 0, 1)\n",
    "    accuracy_test_reglog = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = clasificacion,\n",
    "            normalize = True\n",
    "           )\n",
    "    print(f\"El accuracy de testeo en Reg Log es: {100*accuracy_test_reglog}%\")\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    ## SUPPORT VECTOR MACHINE (SVM)\n",
    "    \n",
    "    #Creacion y entrenamiento del modelo SVM\n",
    "    svm_model=SVC(random_state=100)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    \n",
    "    #Prediccion y accuracy en testeo:\n",
    "    pred_test_svm=svm_model.predict(X_test)\n",
    "    accuracy_test_svm = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = pred_test_svm,\n",
    "            normalize = True\n",
    "           )\n",
    "    print(f\"El accuracy de testeo en SVM es: {100*accuracy_test_svm}%\")\n",
    "    \n",
    "    \n",
    "    ##RANDOM FOREST:\n",
    "    \n",
    "    #Creacion y entrenamiento del modelo Random Forest\n",
    "    random_forest = RandomForestClassifier(\n",
    "            n_estimators = 100,\n",
    "            max_depth    = None,\n",
    "            max_features = 'auto',\n",
    "            oob_score    = False,\n",
    "            n_jobs       = -1,\n",
    "            random_state = 100\n",
    "         )\n",
    "\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    #Prediccion y accuracy en testeo:\n",
    "    pred_test_rf=random_forest.predict(X_test)\n",
    "    accuracy_test_rf = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = pred_test_rf,\n",
    "            normalize = True\n",
    "           )\n",
    "    print(f\"El accuracy de testeo en Random Forest es: {100*accuracy_test_rf}%\")\n",
    "    \n",
    "    \n",
    "    ## K MEANS:\n",
    "    KNN_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    KNN_model.fit(X_train, y_train)\n",
    "    KNN_test_pred = KNN_model.predict(X_test)\n",
    "    \n",
    "    #Accuracy en testeo\n",
    "    accuracy_test = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = KNN_test_pred,\n",
    "            normalize = True\n",
    "           )\n",
    "    print(f\"El accuracy de testeo en KNN es: {100*accuracy_test}%\")\n",
    "    \n",
    "\n",
    "     #Matriz de confusion Regresion Logistica\n",
    "    matrix1=confusion_matrix(y_test,pred_test_log)\n",
    "    ax1= plt.subplot(2,2,1)\n",
    "    sns.heatmap(matrix1, annot=True, fmt='g', ax=ax1);\n",
    "    # labels, title and ticks\n",
    "    ax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels'); \n",
    "    ax1.set_title('Confusion Matrix Reg Log'); \n",
    "    ax1.xaxis.set_ticklabels(['attack', 'normal']); ax1.yaxis.set_ticklabels(['normal', 'attack']);\n",
    "    \n",
    "    #Matriz de confusion SVM\n",
    "    matrix2=confusion_matrix(y_test,pred_test_svm)\n",
    "    ax2= plt.subplot(2,2,2)\n",
    "    sns.heatmap(matrix2, annot=True, fmt='g', ax=ax2);\n",
    "    # labels, title and ticks\n",
    "    ax2.set_xlabel('Predicted labels');ax2.set_ylabel('True labels'); \n",
    "    ax2.set_title('Confusion Matrix SVM'); \n",
    "    ax2.xaxis.set_ticklabels(['attack', 'normal']); ax2.yaxis.set_ticklabels(['normal', 'attack']);\n",
    "    \n",
    "    #Matriz de confusion Random Forest\n",
    "    matrix3=confusion_matrix(y_test,pred_test_rf)\n",
    "    ax3= plt.subplot(2,2,3)\n",
    "    sns.heatmap(matrix3, annot=True, fmt='g', ax=ax3);\n",
    "    # labels, title and ticks\n",
    "    ax3.set_xlabel('Predicted labels');ax3.set_ylabel('True labels'); \n",
    "    ax3.set_title('Confusion Matrix RF'); \n",
    "    ax3.xaxis.set_ticklabels(['attack', 'normal']); ax3.yaxis.set_ticklabels(['normal', 'attack']);\n",
    "    \n",
    "    #Matriz de confusion KNN\n",
    "    matrix4=confusion_matrix(y_test,KNN_test_pred)\n",
    "    ax4= plt.subplot(2,2,4)\n",
    "    sns.heatmap(matrix4, annot=True, fmt='g', ax=ax4);\n",
    "    # labels, title and ticks\n",
    "    ax4.set_xlabel('Predicted labels');ax4.set_ylabel('True labels'); \n",
    "    ax4.set_title('Confusion Matrix KNN'); \n",
    "    ax4.xaxis.set_ticklabels(['attack', 'normal']); ax4.yaxis.set_ticklabels(['normal', 'attack']);\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(accuracy_score(KNN_prediction, y_test))\n",
    "    #Confusion Matrix and Classification Report give more details about performance\n",
    "    #print(classification_report(KNN_prediction, y_test))\n",
    "    \n",
    "#     from tabulate import tabulate\n",
    "#     print(\"Ok, probando si sale una tabla\")\n",
    "    \n",
    "#     d=[[accuracy_test_reglog,accuracy_test_svm]]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "probando(\"Normal_Operation\",\"Attacked_Operation\",\"Imbalanced_Attacked\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9eeb95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
