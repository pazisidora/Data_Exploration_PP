{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f08aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bokeh\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486263cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La siguiente funcion lee las 3 carpetas de informacion y crea 3 dataframes, luego a cada uno les realiza la ventana movil\n",
    "#para asi fusionarlos en un solo dataframe grande. Despues se escala una base solo con las variables numericas para finalmente \n",
    "#realizar los primeros modelos (regresion lineal y logistica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9eeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probando(file1,file2,file3,delta_movil):\n",
    "    \n",
    "    ######Cargar bases NORMAL, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_normal_files=os.listdir(file1)\n",
    "    #Consolidando todas las bases con comportamiento normal en un DF\n",
    "    consolidated_normal_df = pd.DataFrame()\n",
    "    for name in names_normal_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = F'{file1}/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_normal_df = pd.concat([consolidated_normal_df,filename])\n",
    "    \n",
    "    #Asignando etiquetas\n",
    "    consolidated_normal_df['Label']=0\n",
    "    consolidated_normal_df['dataset']=\"Normal\"\n",
    "    \n",
    "    #Ventana movil\n",
    "    data_auxN=pd.DataFrame.copy(consolidated_normal_df)\n",
    "    data_auxN['Time'] = data_auxN.index if data_auxN.index.is_monotonic_increasing else range(len(data_auxN))\n",
    "    data_auxN.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxN2 = pd.DataFrame.copy(consolidated_normal_df)\n",
    "        data_auxN2['Time']= (data_auxN2.index if data_auxN2.index.is_monotonic_increasing else range(len(data_auxN2)))\n",
    "        data_auxN2['Time']=data_auxN2['Time']+j\n",
    "        data_auxN2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j), 'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxN3 = data_auxN.merge(data_auxN2, on='Time', how='inner')\n",
    "        data_auxN = pd.DataFrame.copy(data_auxN3)\n",
    "    #return data_auxN\n",
    "\n",
    "    \n",
    "    ######Cargar bases ATAQUE, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_attacked_files=os.listdir(file2) \n",
    "    #Consolidando todas las bases con comportamiento atacado en un DF\n",
    "    consolidated_attacked_df = pd.DataFrame()\n",
    "    for name in names_attacked_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = f'Attacked_Operation/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_attacked_df = pd.concat([consolidated_attacked_df,filename])\n",
    "    \n",
    "    #Funcion para etiquetar si la medición esta bajo ataque o no (sirve para este item como para las bases desbalanceadas)\n",
    "    def label_data(time):\n",
    "        if time <= 0.3:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    consolidated_attacked_df['Label']=consolidated_attacked_df['Time'].apply(label_data)\n",
    "\n",
    "    #Por temas de testeo, no necesitamos datos despues de los 0,5 segundos por lo que deben ser removidos.\n",
    "    consolidated_attacked_df.reset_index(drop=True,inplace=True)\n",
    "    boolean_mask = consolidated_attacked_df['Time']>0.5\n",
    "    attacked_index = consolidated_attacked_df[boolean_mask].index\n",
    "    consolidated_attacked_df.drop(attacked_index, axis=0,inplace=True)\n",
    "    \n",
    "    #Ahora etiqueto que es una base con ataque\n",
    "    consolidated_attacked_df['dataset'] = \"Attacked\"\n",
    "\n",
    "    #Ventana móvil\n",
    "    data_auxA=pd.DataFrame.copy(consolidated_attacked_df)\n",
    "    data_auxA['Time'] = data_auxA.index if data_auxA.index.is_monotonic_increasing else range(len(data_auxA))\n",
    "    data_auxA.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxA2 = pd.DataFrame.copy(consolidated_attacked_df)\n",
    "        data_auxA2['Time']= (data_auxA2.index if data_auxA2.index.is_monotonic_increasing else range(len(data_auxA2)))\n",
    "        data_auxA2['Time']=data_auxA2['Time']+j\n",
    "        data_auxA2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j),'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxA3 = data_auxA.merge(data_auxA2, on='Time', how='inner')\n",
    "        data_auxA = pd.DataFrame.copy(data_auxA3)\n",
    "    #return data_auxA\n",
    "    \n",
    "    ######Cargar bases DESBALANCEADAS ATACADAS, asignar etiqueta y ventana de tiempo.---------------------------------------------------------------------\n",
    "    names_imbalanced_files=os.listdir(file3)\n",
    "    #Consolidando todas las bases desbalanceadas con comportamiento atacado en un DF\n",
    "    consolidated_imbalanced_df = pd.DataFrame()\n",
    "    for name in names_imbalanced_files:\n",
    "        filename= f'df_{name[:-4]}'\n",
    "        path = f'Imbalanced_Attacked/{name}'\n",
    "        filename = pd.read_csv(path)\n",
    "        consolidated_imbalanced_df = pd.concat([consolidated_imbalanced_df,filename])\n",
    "    \n",
    "    #Aplico etiqueta si esta bajo ataque o no\n",
    "    consolidated_imbalanced_df['Label']=consolidated_imbalanced_df['Time'].apply(label_data)\n",
    "    \n",
    "    #Por temas de testeo, no necesitamos datos despues de los 0,5 segundos por lo que deben ser removidos.\n",
    "    consolidated_imbalanced_df.reset_index(drop=True,inplace=True)\n",
    "    boolean_im_mask = consolidated_imbalanced_df['Time']>0.5\n",
    "    imbalanced_index = consolidated_imbalanced_df[boolean_im_mask].index\n",
    "    consolidated_imbalanced_df.drop(imbalanced_index,axis=0,inplace=True)\n",
    "    \n",
    "    #Ahora etiqueto que es una base desbalanceada con ataque\n",
    "    consolidated_imbalanced_df['dataset'] = \"Imbalanced\"\n",
    "    \n",
    "    #Ventana móvil\n",
    "    data_auxIA=pd.DataFrame.copy(consolidated_imbalanced_df)\n",
    "    data_auxIA['Time'] = data_auxIA.index if data_auxIA.index.is_monotonic_increasing else range(len(data_auxIA))\n",
    "    data_auxIA.rename(columns={'Vm1:Measured voltage:1': 'V1', 'Vm1:Measured voltage:2':'V2', \n",
    "                                          'Vm1:Measured voltage:3':'V3','Divide18:1': 'IM1','Divide18:2':'IM2','Divide18:3': 'IM3',\n",
    "                                          'Am1:Measured current': 'Corriente'}, inplace=True)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        data_auxIA2 = pd.DataFrame.copy(consolidated_imbalanced_df)\n",
    "        data_auxIA2['Time']= (data_auxIA2.index if data_auxIA2.index.is_monotonic_increasing else range(len(data_auxIA2)))\n",
    "        data_auxIA2['Time']=data_auxIA2['Time']+j\n",
    "        data_auxIA2.rename(columns={'Vm1:Measured voltage:1': 'V1_'+str(j), 'Vm1:Measured voltage:2':'V2_'+str(j), \n",
    "                                            'Vm1:Measured voltage:3':'V3_'+str(j),'Divide18:1': 'IM1_'+str(j),'Divide18:2':'IM2_'+str(j),\n",
    "                                            'Divide18:3': 'IM3_'+str(j),'Am1:Measured current': 'Corriente_'+str(j),'Label': 'Label_'+str(j),\n",
    "                                            'dataset': 'dataset_'+str(j)}, inplace=True)\n",
    "        data_auxIA3 = data_auxIA.merge(data_auxIA2, on='Time', how='inner')\n",
    "        data_auxIA = pd.DataFrame.copy(data_auxIA3)\n",
    "    #return data_auxIA    \n",
    "\n",
    "    ###### Consolidando todos los dataframes, con ventana de tiempo, en uno--------------------------------------------------------\n",
    "    consolidated_df = pd.concat([data_auxN, data_auxA]) #bases normales con atacadas\n",
    "    consolidated_df = pd.concat([consolidated_df, data_auxIA]) #lo anterior + desbalanceado\n",
    "    #print(consolidated_df.Label.value_counts())\n",
    "    \n",
    "    #####Reseteando los indices\n",
    "    consolidated_df.reset_index(drop=True,inplace=True)\n",
    "    #print(consolidated_df)\n",
    "    \n",
    "    \n",
    "    #ESTO ES LA PRUEBA TRATANDO DE ESCALAR, EN LA SIGUIENTE LINEA DE CODIGO VEREMOS QUE PASA CON EL MODELO SIN ESCALAR.\n",
    "    \n",
    "    \n",
    "    ##### Preporcesamiento de la data\n",
    "    #Importando para escalar\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler=StandardScaler()\n",
    "    \n",
    "    #Solo se necesita escalar las variables numericas, asi que se realiza eso\n",
    "    numerical_consolidated_df=consolidated_df.drop(['Time','Label','dataset'],axis=1)\n",
    "    \n",
    "    numerical_consolidated_df2=pd.DataFrame.copy(numerical_consolidated_df)\n",
    "    \n",
    "    for j in range(1,delta_movil+1):\n",
    "        numerical_consolidated_df2=numerical_consolidated_df2.drop(['dataset_'+str(j),'Label_'+str(j)],axis=1)\n",
    "    #return numerical_consolidated_df2\n",
    "    \n",
    "    #Escalar solo los valores numericos y colocar de vuelta a un df lo que era string\n",
    "    scaled_consolidated_df=scaler.fit_transform(numerical_consolidated_df2)\n",
    "    scaled_consolidated_df = pd.DataFrame(scaled_consolidated_df,columns=numerical_consolidated_df2.columns)\n",
    "    #print(scaled_consolidated_df)\n",
    "    \n",
    "    #Volver a concatenar las columnas de string\n",
    "    consolidated_df2=pd.concat([scaled_consolidated_df,consolidated_df[['Time','Label','dataset']]],axis=1)\n",
    "    #print(consolidated_df2)\n",
    "    for j in range(1,delta_movil+1):\n",
    "        consolidated_df2=pd.concat([consolidated_df2,consolidated_df[['dataset_'+str(j),'Label_'+str(j)]]],axis=1)\n",
    "    #return consolidated_df2\n",
    "    \n",
    "    \n",
    "    ######### Testing First models -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "    ##### Train, test, split the data -----------------------------------------------------------------------------------------\n",
    "    X=scaled_consolidated_df \n",
    "    y=consolidated_df2['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=100)\n",
    "    \n",
    "    ##Regresion LOGISTICA:____________________________________\n",
    "    \n",
    "    #Creacion y entrenamiento del modelo logístico\n",
    "    reg_log = LogisticRegression() #cargo el modelo de reg log\n",
    "    reg_log.fit(X_train, y_train) #Entreno el clasificador\n",
    "    \n",
    "    #print(\"Valor del intercepto:\", reg_log.intercept_)\n",
    "    #print(\"Valores de los Coeficientes:\", list(zip(X.columns, reg_log.coef_.flatten(), )))\n",
    "    \n",
    "    #Prediccion y Accuracy de testeo\n",
    "    #X_test = sm.add_constant(X_test, prepend=True) #no se en realidad si esto se debe agregar o no\n",
    "    pred_test_log = reg_log.predict(X_test)\n",
    "    clasificacion = np.where(pred_test_log<0.5, 0, 1)\n",
    "    accuracy_test_reglog = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = clasificacion,\n",
    "            normalize = True\n",
    "           )\n",
    "    \n",
    "    ## SUPPORT VECTOR MACHINE (SVM):_________________________________\n",
    "    \n",
    "    #Creacion y entrenamiento del modelo SVM\n",
    "    svm_model=SVC(random_state=100,probability=True)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    \n",
    "    #Prediccion y accuracy en testeo:\n",
    "    pred_test_svm=svm_model.predict(X_test)\n",
    "    accuracy_test_svm = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = pred_test_svm,\n",
    "            normalize = True\n",
    "           )\n",
    "    \n",
    "    ##RANDOM FOREST:____________________________________\n",
    "    \n",
    "    #Creacion y entrenamiento del modelo Random Forest\n",
    "    random_forest = RandomForestClassifier(\n",
    "            n_estimators = 100,\n",
    "            max_depth    = None,\n",
    "            max_features = 'auto',\n",
    "            oob_score    = False,\n",
    "            n_jobs       = -1,\n",
    "            random_state = 100\n",
    "         )\n",
    "\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    \n",
    "    #Prediccion y accuracy en testeo:\n",
    "    pred_test_rf=random_forest.predict(X_test)\n",
    "    accuracy_test_rf = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = pred_test_rf,\n",
    "            normalize = True\n",
    "           )\n",
    "    \n",
    "    \n",
    "    ## K MEANS:___________________________________________________\n",
    "    KNN_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    KNN_model.fit(X_train, y_train)\n",
    "    KNN_test_pred = KNN_model.predict(X_test)\n",
    "    \n",
    "    #Accuracy en testeo\n",
    "    accuracy_test_knn = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = KNN_test_pred,\n",
    "            normalize = True\n",
    "           )\n",
    "    \n",
    "    #Matrices\n",
    "    #Matriz de confusion Regresion Logistica\n",
    "    matrix1=confusion_matrix(y_test,pred_test_log)\n",
    "    ax1= plt.subplot(2,2,1)\n",
    "    sns.heatmap(matrix1, annot=True, fmt='g', ax=ax1);\n",
    "    # labels, title and ticks\n",
    "    ax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels'); \n",
    "    ax1.set_title('Confusion Matrix Reg Log'); \n",
    "    ax1.xaxis.set_ticklabels(['normal','attack']); ax1.yaxis.set_ticklabels(['normal', 'attack']);\n",
    "    \n",
    "    #Matriz de confusion SVM\n",
    "    matrix2=confusion_matrix(y_test,pred_test_svm)\n",
    "    ax2= plt.subplot(2,2,2)\n",
    "    sns.heatmap(matrix2, annot=True, fmt='g', ax=ax2);\n",
    "    # labels, title and ticks\n",
    "    ax2.set_xlabel('Predicted labels');ax2.set_ylabel('True labels'); \n",
    "    ax2.set_title('Confusion Matrix SVM'); \n",
    "    ax2.xaxis.set_ticklabels(['normal','attack']); ax2.yaxis.set_ticklabels(['normal', 'attack']);\n",
    "    \n",
    "    #Matriz de confusion Random Forest\n",
    "    matrix3=confusion_matrix(y_test,pred_test_rf)\n",
    "    ax3= plt.subplot(2,2,3)\n",
    "    sns.heatmap(matrix3, annot=True, fmt='g', ax=ax3);\n",
    "    # labels, title and ticks\n",
    "    ax3.set_xlabel('Predicted labels');ax3.set_ylabel('True labels'); \n",
    "    ax3.set_title('Confusion Matrix RF'); \n",
    "    ax3.xaxis.set_ticklabels(['normal','attack']); ax3.yaxis.set_ticklabels(['normal', 'attack']);\n",
    "    \n",
    "    #Matriz de confusion KNN\n",
    "    matrix4=confusion_matrix(y_test,KNN_test_pred)\n",
    "    ax4= plt.subplot(2,2,4)\n",
    "    sns.heatmap(matrix4, annot=True, fmt='g', ax=ax4);\n",
    "    # labels, title and ticks\n",
    "    ax4.set_xlabel('Predicted labels');ax4.set_ylabel('True labels'); \n",
    "    ax4.set_title('Confusion Matrix KNN'); \n",
    "    ax4.xaxis.set_ticklabels(['normal','attack']); ax4.yaxis.set_ticklabels(['normal', 'attack']);\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #AUC-----------------------------------------------------------------------------------------------------\n",
    "    r_probs = [0 for _ in range(len(y_test))]\n",
    "    \n",
    "    reglog_probs = reg_log.predict_proba(X_test)\n",
    "    svm_probs = svm_model.predict_proba(X_test) #nota: para poder realizar esto con SVM tuve que cambiar en la funcion de SVM \n",
    "    #que la probabilidad de la funcion sea True\n",
    "    rf_probs = random_forest.predict_proba(X_test)\n",
    "    knn_probs = KNN_model.predict_proba(X_test)\n",
    "    \n",
    "    reglog_probs = reglog_probs[:,1]\n",
    "    svm_probs = svm_probs[:,1]\n",
    "    rf_probs = rf_probs[:,1]\n",
    "    knn_probs = knn_probs[:,1]\n",
    "    \n",
    "    r_auc = roc_auc_score(y_test,r_probs)\n",
    "    reglog_auc=roc_auc_score(y_test,reglog_probs)\n",
    "    svm_auc = roc_auc_score(y_test,svm_probs)\n",
    "    rf_auc = roc_auc_score(y_test,rf_probs)\n",
    "    knn_auc = roc_auc_score(y_test, knn_probs)\n",
    "    \n",
    "    #ROC curve-----------------------------------------------------------------------------------------------------\n",
    "    #FPR es la tasa de falsos postivos mientras que TPR es la tasa de verdaderos positivos.\n",
    "    r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n",
    "    reglog_fpr, reglog_tpr, _ =roc_curve(y_test, reglog_probs)\n",
    "    svm_fpr, svm_tpr, _ = roc_curve(y_test, svm_probs)\n",
    "    rf_fpr, rf_tpr, _ =roc_curve(y_test,rf_probs)\n",
    "    knn_fpr, knn_tpr, _ = roc_curve(y_test, knn_probs)\n",
    "    \n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(r_fpr,r_tpr,linestyle='--', label='Random Prediction (AUC = %0.4f)' % r_auc)\n",
    "    plt.plot(reglog_fpr,reglog_tpr,marker='.', label='Regresion logistica (AUC = %0.4f)' % reglog_auc)\n",
    "    plt.plot(svm_fpr,svm_tpr,marker='.', label='SVM (AUC = %0.4f)' % svm_auc)\n",
    "    plt.plot(rf_fpr,rf_tpr,marker='.', label='Random Forest (AUC = %0.4f)' % rf_auc)\n",
    "    plt.plot(knn_fpr,knn_tpr,marker='.', label='KNN (AUC = %0.4f)' % knn_auc)\n",
    "\n",
    "    #title\n",
    "    plt.title('ROC Plot')\n",
    "    #Ejes\n",
    "    plt.xlabel('False positive Rate')\n",
    "    plt.ylabel('True positive Rate')\n",
    "    #Show legend\n",
    "    plt.legend()\n",
    "    #Show plot\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #TABLA RESUMEN\n",
    "    print(\"Machine Learning\")\n",
    "    print(tabulate([[\" \", \"Accuracy\", \"ROC AUC\"],\n",
    "                    [\"Reg logistica\", accuracy_test_reglog,reglog_auc],\n",
    "                    [\"SVM\",accuracy_test_svm,svm_auc],\n",
    "                    [\"Random Forest\",accuracy_test_rf,rf_auc],\n",
    "                    [\"KNN\",accuracy_test_knn,knn_auc]], \n",
    "                   headers=\"firstrow\",tablefmt=\"github\"))\n",
    "\n",
    "probando(\"Normal_Operation\",\"Attacked_Operation\",\"Imbalanced_Attacked\",2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
